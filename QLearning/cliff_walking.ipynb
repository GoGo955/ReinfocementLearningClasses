{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import GridWorld\n",
    "\n",
    "%matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "UP = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "LEFT = 3\n",
    "actions = ['UP', 'DOWN', 'RIGHT', 'LEFT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_greedy_policy(q_values, state, epsilon=0.1):\n",
    "    ''' \n",
    "    Choose an action based on a epsilon greedy policy.    \n",
    "    A random action is selected with epsilon probability, else select the best action.    \n",
    "    '''\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.choice(4)\n",
    "    else:\n",
    "        return np.argmax(q_values[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, num_episodes=1000, render=True, exploration_rate=0.1,\n",
    "               learning_rate=0.5, gamma=0.9):    \n",
    "    q_values = np.zeros((4*12, 4))\n",
    "    ep_rewards = []\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        state = env.reset()    \n",
    "        done = False\n",
    "        reward_sum = 0\n",
    "\n",
    "        while not done:            \n",
    "            # Choose action        \n",
    "            action = select_greedy_policy(q_values, state, exploration_rate)\n",
    "            # Do the action\n",
    "            next_state, reward, done = env.step(action)\n",
    "            reward_sum += reward\n",
    "            # Update q_values       \n",
    "            td_target = reward + 0.9 * np.max(q_values[next_state])\n",
    "            td_error = td_target - q_values[state][action]\n",
    "            q_values[state][action] += learning_rate * td_error\n",
    "            # Update state\n",
    "            state = next_state\n",
    "\n",
    "            if render:\n",
    "                env.render(q_values, action=actions[action], colorize_q=True)\n",
    "            \n",
    "        ep_rewards.append(reward_sum)\n",
    "    \n",
    "    return ep_rewards, q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_learning_rewards, q_values = q_learning(env, gamma=0.9, learning_rate=1, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env, num_episodes=1000, render=True, exploration_rate=0.1,\n",
    "          learning_rate=0.5, gamma=0.9):\n",
    "    q_values_sarsa = np.zeros((4*12, 4))\n",
    "    ep_rewards = []\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        state = env.reset()    \n",
    "        done = False\n",
    "        reward_sum = 0\n",
    "        # Choose action        \n",
    "        action = select_greedy_policy(q_values_sarsa, state, exploration_rate)\n",
    "\n",
    "        while not done:        \n",
    "            # Do the action\n",
    "            next_state, reward, done = env.step(action)\n",
    "            reward_sum += reward\n",
    "            \n",
    "            # Choose next action\n",
    "            next_action = select_greedy_policy(q_values_sarsa, next_state, exploration_rate)\n",
    "            # Next q value is the value of the next action\n",
    "            td_target = reward + gamma * q_values_sarsa[next_state][next_action]\n",
    "            td_error = td_target - q_values_sarsa[state][action]\n",
    "            # Update q value\n",
    "            q_values_sarsa[state][action] += learning_rate * td_error\n",
    "\n",
    "            # Update state and action        \n",
    "            state = next_state\n",
    "            action = next_action\n",
    "            \n",
    "            if render:\n",
    "                env.render(q_values, action=actions[action], colorize_q=True)\n",
    "                \n",
    "        ep_rewards.append(reward_sum)\n",
    "    return ep_rewards, q_values_sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa_rewards, q_values_sarsa = sarsa(env, render=True, learning_rate=0.5, gamma=0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reward(values, is_q_learning = True):\n",
    "    env.render(values, colorize_q=True)\n",
    "    \n",
    "    if is_q_learning:\n",
    "        rewards, _ = zip(*[q_learning(env, render=False, exploration_rate=0.1, learning_rate=1) for _ in range(10)])\n",
    "    else:\n",
    "        rewards, _ = zip(*[sarsa(env, render=False, exploration_rate=0.2) for _ in range(100)])\n",
    "    \n",
    "    avg_rewards = np.mean(rewards, axis=0)\n",
    "    mean_reward = [np.mean(avg_rewards)] * len(avg_rewards)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlabel('Episodes')\n",
    "    ax.set_ylabel('Rewards')\n",
    "    ax.plot(avg_rewards)\n",
    "    ax.plot(mean_reward, 'g--')\n",
    "\n",
    "    print('Mean Reward: {}'.format(mean_reward[0]))\n",
    "\n",
    "def play(q_values):\n",
    "    env = GridWorld()\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:    \n",
    "        # Select action\n",
    "        action = select_greedy_policy(q_values, state, 0.0)\n",
    "        # Do the action\n",
    "        next_state, reward, done = env.step(action)  \n",
    "\n",
    "        # Update state and action        \n",
    "        state = next_state  \n",
    "        \n",
    "        env.render(q_values=q_values, action=actions[action], colorize_q=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward: -38.8523\n"
     ]
    }
   ],
   "source": [
    "plot_reward(q_values)\n",
    "play(q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward: -84.68666\n"
     ]
    }
   ],
   "source": [
    "plot_reward(values = q_values_sarsa, is_q_learning = False)\n",
    "play(q_values_sarsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
